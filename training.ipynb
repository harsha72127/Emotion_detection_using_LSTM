{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2b5cfd-cb79-49f6-8262-57cefdea515f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.3)\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting sounddevice\n",
      "  Downloading sounddevice-0.5.3-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting pyaudio\n",
      "  Downloading PyAudio-0.2.14-cp312-cp312-win_amd64.whl.metadata (2.7 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.63.1-cp312-cp312-win_amd64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (1.16.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-1.0.0-cp312-abi3-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (4.15.0)\n",
      "Collecting lazy_loader>=0.1 (from librosa)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.2-cp312-cp312-win_amd64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sounddevice) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.23)\n",
      "Requirement already satisfied: packaging in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Collecting llvmlite<0.47,>=0.46.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.46.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pooch>=1.1->librosa) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.8.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Downloading sounddevice-0.5.3-py3-none-win_amd64.whl (364 kB)\n",
      "Downloading PyAudio-0.2.14-cp312-cp312-win_amd64.whl (164 kB)\n",
      "Downloading audioread-3.1.0-py3-none-any.whl (23 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.2-cp312-cp312-win_amd64.whl (72 kB)\n",
      "Downloading numba-0.63.1-cp312-cp312-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 1.6/2.8 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 6.4 MB/s  0:00:00\n",
      "Downloading llvmlite-0.46.0-cp312-cp312-win_amd64.whl (38.1 MB)\n",
      "   ---------------------------------------- 0.0/38.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/38.1 MB 3.4 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 1.3/38.1 MB 3.0 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 1.8/38.1 MB 3.1 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 2.6/38.1 MB 3.4 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 3.7/38.1 MB 3.6 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 4.5/38.1 MB 3.7 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 5.5/38.1 MB 3.9 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 6.6/38.1 MB 4.0 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 7.3/38.1 MB 4.1 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 8.7/38.1 MB 4.3 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 9.7/38.1 MB 4.4 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 11.0/38.1 MB 4.6 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 12.3/38.1 MB 4.7 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 13.9/38.1 MB 4.9 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 15.2/38.1 MB 5.1 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 16.5/38.1 MB 5.1 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 17.3/38.1 MB 5.1 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 17.8/38.1 MB 5.0 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 18.6/38.1 MB 4.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 19.4/38.1 MB 4.8 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 19.9/38.1 MB 4.7 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 21.0/38.1 MB 4.7 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 21.8/38.1 MB 4.7 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 22.8/38.1 MB 4.7 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 23.9/38.1 MB 4.7 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 24.9/38.1 MB 4.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 26.0/38.1 MB 4.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 27.3/38.1 MB 4.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 28.6/38.1 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 30.1/38.1 MB 4.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 31.5/38.1 MB 5.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 32.8/38.1 MB 5.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 34.1/38.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.1/38.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.2/38.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  37.5/38.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.1/38.1 MB 5.1 MB/s  0:00:07\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 5.4 MB/s  0:00:00\n",
      "Downloading soxr-1.0.0-cp312-abi3-win_amd64.whl (172 kB)\n",
      "Installing collected packages: pyaudio, soxr, msgpack, llvmlite, lazy_loader, audioread, soundfile, sounddevice, pooch, numba, librosa\n",
      "\n",
      "   ---------- -----------------------------  3/11 [llvmlite]\n",
      "   ---------- -----------------------------  3/11 [llvmlite]\n",
      "   ---------- -----------------------------  3/11 [llvmlite]\n",
      "   ---------- -----------------------------  3/11 [llvmlite]\n",
      "   ---------- -----------------------------  3/11 [llvmlite]\n",
      "   ---------- -----------------------------  3/11 [llvmlite]\n",
      "   ---------- -----------------------------  3/11 [llvmlite]\n",
      "   -------------- -------------------------  4/11 [lazy_loader]\n",
      "   --------------------- ------------------  6/11 [soundfile]\n",
      "   ----------------------------- ----------  8/11 [pooch]\n",
      "   ----------------------------- ----------  8/11 [pooch]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   -------------------------------- -------  9/11 [numba]\n",
      "   ------------------------------------ --- 10/11 [librosa]\n",
      "   ------------------------------------ --- 10/11 [librosa]\n",
      "   ------------------------------------ --- 10/11 [librosa]\n",
      "   ---------------------------------------- 11/11 [librosa]\n",
      "\n",
      "Successfully installed audioread-3.1.0 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.46.0 msgpack-1.1.2 numba-0.63.1 pooch-1.8.2 pyaudio-0.2.14 sounddevice-0.5.3 soundfile-0.13.1 soxr-1.0.0\n",
      "Requirement already satisfied: torch in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: filelock in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cheda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy librosa sounddevice pyaudio\n",
    "!pip install torch torchaudio scikit-learn matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9fc973e-de52-4091-b9ba-18ea2de76bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36a5374-6614-413f-b134-6719fd5f6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_map={\n",
    "    \"01\":\"neutral\",\"02\":\"calm\",\"03\":\"happy\",\"04\":\"sad\",\n",
    "    \"05\":\"angry\",\"06\":\"fearful\",\"07\":\"disgust\",\"08\":\"surprised\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed14b61-bb7b-4dc5-90da-79a3bd999596",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"./data/ravdess\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd928f7f-a8f4-4457-9e91-033c1e34ce50",
   "metadata": {},
   "source": [
    "### checking if the path is exists or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a8f2ce-b6fe-4c74-b5d5-a9a0f8603b4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c249c83-4178-4bd3-9682-6a38c65b95ed",
   "metadata": {},
   "source": [
    "This defines a function named extract_mfcc.\n",
    "\n",
    "It takes two parameters:\n",
    "\n",
    "file_path: the path to the audio file you want to process.\n",
    "\n",
    "n_mfcc=40: optional argument specifying the number of MFCC coefficients to extract (default = 40).\n",
    "librosa.load() loads the audio file from the given file_path.\n",
    "\n",
    "signal is the actual audio waveform stored as a NumPy array. It contains amplitude values of the sound over time.\n",
    "\n",
    "sr is the sampling rate (how many samples per second).\n",
    "\n",
    "By setting sr=22050, you force the audio to be resampled to 22,050 Hz (a common standard in speech/audio processing). If the file has a different sampling rate, it will be converted to 22050.\n",
    "librosa.feature.mfcc() computes the Mel Frequency Cepstral Coefficients from the audio signal.\n",
    "\n",
    "y=signal: the waveform to analyze.\n",
    "\n",
    "sr=sr: sampling rate used for correct frequency calculations.\n",
    "\n",
    "n_mfcc=n_mfcc: number of MFCC features (rows) to extract.\n",
    "\n",
    "Output: a 2-D NumPy array of shape (n_mfcc, time_frames)\n",
    "\n",
    "Each row = 1 MFCC coefficient\n",
    "\n",
    "Each column = value of that coefficient at a specific time frame.\n",
    "\n",
    "MFCCs are compact features representing how humans perceive sound, commonly used in speech emotion recognition, speech recognition, etc.\n",
    "mfcc.T transposes the MFCC matrix so shape becomes (time_frames, n_mfcc).\n",
    "\n",
    "Taking np.mean(..., axis=0) means:\n",
    "\n",
    "You compute the average of each MFCC coefficient across all time frames.\n",
    "\n",
    "This compresses the 2-D MFCC feature matrix into a single 1-D feature vector of length n_mfcc.\n",
    "\n",
    "Essentially, you summarize the entire audio clip into an average representation instead of a frame-by-frame series.\n",
    "Returns the final MFCC feature vector.\n",
    "\n",
    "Output shape: (n_mfcc,) — a single vector containing averaged MFCC values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25d8b226-1c40-44f8-8a30-d13ac5fa2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(file_path,n_mfcc=40):\n",
    "    signal,sr=librosa.load(file_path,sr=220500)\n",
    "    mfcc=librosa.feature.mfcc(y=signal,sr=sr,n_mfcc=n_mfcc)\n",
    "    mfcc=np.mean(mfcc.T,axis=0)\n",
    "    return mfcc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44297fc-1fd9-4b06-9db2-53a7b8a65c70",
   "metadata": {},
   "source": [
    "os.walk(DATA) recursively walks through all folders and subfolders inside the directory DATA.\n",
    "\n",
    "It returns:\n",
    "\n",
    "root → the current folder path being scanned\n",
    "\n",
    "dirs → list of subdirectories inside the current folder\n",
    "\n",
    "files → list of files in the current folder\n",
    "\n",
    "The loop iterates over every folder and every file inside your dataset directory.\n",
    "\n",
    "Iterates over each file discovered in the current root directory.\n",
    "Ensures you only process audio files with extension .wav.\n",
    "\n",
    "Skips any non-audio files (like metadata, readme files, etc.).\n",
    "RAVDESS filenames follow a structured format like:\n",
    "03-01-05-01-02-02-12.wav\n",
    "\n",
    "file.split(\"-\") breaks the filename into parts:\n",
    "[2] selects the 3rd element from the split list (indexing starts at 0).\n",
    "\n",
    "That element represents the emotion code in the RAVDESS dataset.\n",
    "\n",
    "For example:\n",
    "\n",
    "01 = neutral\n",
    "\n",
    "02 = calm\n",
    "\n",
    "03 = happy\n",
    "\n",
    "04 = sad\n",
    "\n",
    "05 = angry\n",
    "\n",
    "06 = fearful\n",
    "\n",
    "07 = disgust\n",
    "\n",
    "08 = surprised\n",
    "\n",
    "(Your EMOTION_MAP handles the exact mapping.)\n",
    "os.path.join(root, file) builds the full path to the audio file.\n",
    "\n",
    "extract_mfcc(...) is your function that:\n",
    "\n",
    "Loads the audio file\n",
    "\n",
    "Extracts MFCC features\n",
    "\n",
    "Returns a feature vector\n",
    "\n",
    "The output features is typically a 1-D NumPy array of length n_mfcc (e.g., 40).\n",
    "X is your features dataset list.\n",
    "\n",
    "Each entry corresponds to one audio sample’s MFCC feature vector.\n",
    "\n",
    "Over the loop, X becomes a list of feature vectors.\n",
    "y is your labels list.\n",
    "\n",
    "For every feature vector in X, you store the corresponding target emotion label in y.\n",
    "\n",
    "So at the end:\n",
    "\n",
    "X[i] contains MFCC features of a speech file\n",
    "\n",
    "y[i] contains the emotion label of that same file\n",
    "\n",
    "Both lists stay aligned.\n",
    "Converts the Python lists into NumPy arrays.\n",
    "\n",
    "This is necessary because machine learning models expect NumPy arrays.\n",
    "\n",
    "Final shapes usually:\n",
    "\n",
    "X.shape → (num_samples, num_features)\n",
    "\n",
    "y.shape → (num_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a126aae-0731-453b-9d3a-9b532c571830",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=[],[]\n",
    "DATA=\"data/ravdess\"\n",
    "for root, dirs,files in os.walk(DATA):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            emotion_code=file.split(\"-\")[2]\n",
    "            emotion=emotion_map[emotion_code]\n",
    "            features=extract_mfcc(os.path.join(root,file))\n",
    "            X.append(features)\n",
    "            y.append(emotion)\n",
    "X=np.array(X)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b73b7ebb-34ee-4100-a55d-9f5382a57cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c8628eb-789d-4147-a02a-a15f38a4c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=LabelEncoder()\n",
    "y=encoder.fit_transform(y)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "X_train=torch.tensor(X_train,dtype=torch.float32)\n",
    "y_train=torch.tensor(y_train,dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e625e-8eb7-408c-91ba-bfa4247fb082",
   "metadata": {},
   "source": [
    "Imports the required libraries.\n",
    "\n",
    "torch and torch.nn are used to build and train the neural network.\n",
    "\n",
    "LabelEncoder converts emotion labels (like “happy”, “sad”) into numeric values.\n",
    "\n",
    "train_test_split splits the dataset into training and testing subsets.\n",
    "Creates an instance of LabelEncoder.\n",
    "\n",
    "This encoder will map each unique emotion label to an integer.\n",
    "Fits the encoder on all labels in y and transforms them into numeric values.\n",
    "\n",
    "Example:\n",
    "\n",
    "[\"happy\",\"sad\",\"angry\"] → [2,1,0]\n",
    "\n",
    "Now the target labels are integers, which neural networks require.\n",
    "Splits dataset into:\n",
    "\n",
    "X_train, y_train → training data\n",
    "\n",
    "X_test, y_test → testing data\n",
    "\n",
    "test_size=0.2 means 20% of data is kept for testing and 80% for training.\n",
    "\n",
    "Ensures your model is evaluated on unseen data.\n",
    "Converts the NumPy array X_train into a PyTorch tensor.\n",
    "\n",
    "dtype=torch.float32 ensures values are floating-point numbers.\n",
    "\n",
    "Neural networks require tensor input, not NumPy arrays.\n",
    "Converts training labels to PyTorch tensor.\n",
    "\n",
    "dtype=torch.long is required because CrossEntropyLoss() expects labels as long integers (class indices).\n",
    "Defines a new neural network class named LSTMEmotion.\n",
    "\n",
    "It inherits from nn.Module, meaning it is a PyTorch neural network.\n",
    "Constructor for the class.\n",
    "\n",
    "super().__init__() initializes the parent nn.Module class properly.\n",
    "Creates an LSTM layer.\n",
    "\n",
    "Inputs:\n",
    "\n",
    "40 → Input feature size. This matches your MFCC vector length (40 MFCC coefficients).\n",
    "\n",
    "128 → Hidden state size (how many features LSTM learns internally).\n",
    "\n",
    "batch_first=True\n",
    "\n",
    "Means input will be in shape: (batch_size, sequence_length, feature_size)\n",
    "\n",
    "Here: (batch, 1, 40) since you will add a sequence dimension later.\n",
    "Fully Connected (dense) layer.\n",
    "\n",
    "Input = 128 (output of LSTM hidden state)\n",
    "\n",
    "Output = 8\n",
    "\n",
    "Because there are 8 possible emotions in RAVDESS dataset.\n",
    "\n",
    "Converts LSTM representation into emotion class scores.\n",
    "Defines how data flows through the network.\n",
    "Adds a sequence dimension.\n",
    "\n",
    "Original x shape: (batch, 40)\n",
    "\n",
    "After unsqueeze: (batch, 1, 40)\n",
    "\n",
    "LSTM expects 3D input: (batch, sequence_length, features)\n",
    "\n",
    "Here each sample is treated as a single-time-step sequence.\n",
    "Passes data through the LSTM.\n",
    "\n",
    "LSTM returns:\n",
    "\n",
    "Output for every time step (ignored using _)\n",
    "\n",
    "Hidden state h and cell state c (you only need hidden state).\n",
    "\n",
    "h shape = (num_layers, batch, hidden_size)\n",
    "\n",
    "You take the last layer’s hidden state.\n",
    "Takes the last hidden state h[-1] (shape: (batch, 128)).\n",
    "\n",
    "Passes it through the fully connected layer.\n",
    "\n",
    "Output shape becomes (batch, 8) → one score per emotion class.\n",
    "\n",
    "Returned values are logits (raw prediction scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9a99ec7-95d5-4993-8a07-42c9fdff87bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEmotion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm=nn.LSTM(40,128,batch_first=True)\n",
    "        self.fc=nn.Linear(128,8)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=x.unsqueeze(1)\n",
    "        _,(h,_)=self.lstm(x)\n",
    "        return self.fc(h[-1])\n",
    "model=LSTMEmotion()\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb4de82-6c15-4d75-b73a-d56f0ef248a7",
   "metadata": {},
   "source": [
    "Starts a loop that runs 50 times, meaning 50 training epochs.\n",
    "\n",
    "One epoch = one full pass of the training dataset through the model.\n",
    "\n",
    "epoch takes values from 0 to 49.\n",
    "Resets (clears) all previously calculated gradients in the model.\n",
    "\n",
    "Gradients accumulate by default in PyTorch.\n",
    "\n",
    "If you do not clear them, gradients from previous backward passes will add up and corrupt learning.\n",
    "\n",
    "This ensures each training step uses only current gradients.\n",
    "Performs backpropagation.\n",
    "\n",
    "Calculates gradients of the loss with respect to every model parameter.\n",
    "\n",
    "These gradients tell the optimizer how to modify weights to reduce loss.\n",
    "Updates model parameters using calculated gradients.\n",
    "\n",
    "Uses Adam optimizer with learning rate 0.001.\n",
    "\n",
    "This is the actual “learning” step where weights change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3587026-992f-4c99-8b76-4349db375a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0012024730676785111\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    output=model(X_train)\n",
    "    loss=criterion(output,y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca10d70c-eb39-4fe5-8036-a9035eae2b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3291e08-eb48-45ab-9f41-1be1235e905f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96        80\n",
      "           1       0.90      0.97      0.93        72\n",
      "           2       0.97      0.92      0.94        74\n",
      "           3       0.91      0.89      0.90        88\n",
      "           4       0.93      0.97      0.95        77\n",
      "           5       0.89      0.89      0.89        37\n",
      "           6       0.91      0.83      0.87        71\n",
      "           7       0.88      0.92      0.90        77\n",
      "\n",
      "    accuracy                           0.92       576\n",
      "   macro avg       0.92      0.92      0.92       576\n",
      "weighted avg       0.92      0.92      0.92       576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_test_t=torch.tensor(X_test,dtype=torch.float32)\n",
    "pred=model(X_test_t)\n",
    "pred=torch.argmax(pred,axis=1)\n",
    "\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f94a3185-728d-449e-9792-4c9c98bf21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b49ec03-9cd5-40f6-8623-33cc3874720b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76  0  0  0  0  0  2  2]\n",
      " [ 0 70  0  0  0  2  0  0]\n",
      " [ 0  2 68  2  2  0  0  0]\n",
      " [ 0  2  2 78  2  0  0  4]\n",
      " [ 0  0  0  0 75  0  0  2]\n",
      " [ 0  2  0  2  0 33  0  0]\n",
      " [ 0  2  0  4  2  2 59  2]\n",
      " [ 2  0  0  0  0  0  4 71]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2fe78-0ee1-4c52-9495-8df10a584ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
