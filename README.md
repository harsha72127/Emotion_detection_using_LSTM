

# ğŸ¤ Real-Time Voice Emotion Detection using Deep Learning (LSTM)

![Python](https://img.shields.io/badge/Python-3.10-blue)
![PyTorch](https://img.shields.io/badge/Framework-PyTorch-EE4C2C)
![Librosa](https://img.shields.io/badge/Audio-Librosa-green)
![Streamlit](https://img.shields.io/badge/UI-Streamlit-red)
![Status](https://img.shields.io/badge/Status-Production_Ready-brightgreen)

A **real-time speech emotion recognition system** powered by **MFCC audio features + LSTM Neural Network**, featuring a **clean modular codebase**, **GUI using Streamlit**, and **support for live microphone recording**.

The system captures microphone input in real time, extracts audio features, predicts emotions using a trained deep learning model, and displays classified emotions along with probability scores.

---

## ğŸ“Œ Key Highlights

âœ” Real-time microphone-based emotion prediction
âœ” Trained on **RAVDESS Emotional Speech Dataset**
âœ” **MFCC Feature Extraction + LSTM Model**
âœ” Highly modular ML pipeline
âœ” Streamlit UI for end users
âœ” Visual emotion probability distribution
âœ” Model evaluation with confusion matrix
âœ” Ready for deployment and portfolio showcase

---

## ğŸ˜ƒ Emotions Detected

* Happy
* Sad
* Angry
* Fearful
* Neutral
* Calm
* Disgust
* Surprise

---

## ğŸ§  Model Overview

| Component         | Description               |
| ----------------- | ------------------------- |
| Input             | Raw voice waveform        |
| Feature Extractor | MFCC via Librosa          |
| Model             | LSTM RNN                  |
| Framework         | PyTorch                   |
| Output            | Emotion Class Probability |

---

## ğŸ“‚ Project Structure

```
emotion_detection
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ training.ipynb
â”‚   â””â”€â”€ real_time_detection.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset_loader.py
â”‚   â”œâ”€â”€ feature_extraction.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ realtime.py
â”‚
â”œâ”€â”€ saved_models/
â”‚   â””â”€â”€ model.pth
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â””â”€â”€ assets/
    â”œâ”€â”€ demo.png
    â””â”€â”€ confusion_matrix.png
```

---

## ğŸ“¥ Dataset

This project uses the **RAVDESS Emotional Speech Audio Dataset**

Download from Kaggle:

[https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio](https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio)

Place it in:

```
emotion_detection/data/ravdess/
```

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Create Virtual Environment

Windows:

```
python -m venv venv
venv\Scripts\activate
```

Linux / Mac:

```
python3 -m venv venv
source venv/bin/activate
```

---

### 2ï¸âƒ£ Install Dependencies

```
pip install -r requirements.txt
```

If PyAudio fails (Windows):

```
pip install pipwin
pipwin install pyaudio
```

If microphone not working, also install:

```
pip install sounddevice
```

---

## ğŸ‹ï¸ Training The Model

Open the notebook:

```
notebooks/training.ipynb
```

It will:

âœ” Load dataset
âœ” Extract MFCC features
âœ” Train LSTM model
âœ” Evaluate performance
âœ” Save trained model to:

```
saved_models/model.pth
```

---

## ğŸ§ Real-Time Voice Detection (Notebook)

To test real-time emotions inside Jupyter:

```
notebooks/real_time_detection.ipynb
```

---

## ğŸ–¥ï¸ Streamlit Application (Main UI)

Run:

```
streamlit run app.py
```

âœ” Select recording duration
âœ” Select sampling rate (default recommended: 22050 Hz)
âœ” Press "Record & Predict Emotion"
âœ” Speak
âœ” View prediction & probability graph

---

## ğŸ“Š Model Evaluation

Confusion Matrix:

```
assets/confusion_matrix.png
```

Application Demo:

```
assets/demo.png
```

---

## ğŸ§° Tech Stack

* Python
* PyTorch
* Librosa
* NumPy
* Scikit-Learn
* Streamlit
* SoundDevice / PyAudio

---

## ğŸš€ Deployment Options

You can deploy using:

* Streamlit Cloud
* HuggingFace Spaces
* Local Desktop App
* Flask Backend + React UI
* Docker Container

---

## â— Troubleshooting

### 1ï¸âƒ£ Librosa Warning: Empty Filters

If you see:

```
Empty filters detected in mel frequency basis
```

Fix:

* Ensure sampling rate = 22050
* Set `fmax = sr // 2`
* Use `n_mels = 40 or 64`

---

### 2ï¸âƒ£ `train_test_split: n_samples = 0`

Means dataset failed to load or features empty.

Check:

```
print(len(X), len(y))
```

Ensure audio is valid & features extracted correctly.

---

### 3ï¸âƒ£ Microphone Not Working

Install:

```
pip install sounddevice
pip install pyaudio
```

Run as administrator if needed.

---

### 4ï¸âƒ£ Very Low Accuracy?

* Train longer
* Increase MFCC features
* Add noise handling
* Normalize audio
* Use more training samples

---

## ğŸ§ª Future Enhancements

* CNN + LSTM Hybrid
* Attention Mechanism
* Noise Resistant Training
* Multi-language support
* Mobile / Desktop App
* Cloud Deployment
* Real-time streaming support

---

## ğŸ‘¤ Author

**Chedalla Harsha**

---

## ğŸ“œ License

This project is licensed under **MIT License**
Free to use, modify and distribute.
