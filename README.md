# Emotion_detection_using_LSTM

# ğŸ¤ Real-Time Emotion Detection from Voice using LSTM (RNN)

This project implements a **real-time human emotion detection system from speech audio** using **Deep Learning (LSTM-based RNN)**. The model is trained on the **RAVDESS Emotional Speech Dataset** and performs live emotion recognition using microphone input.

It demonstrates end-to-end capabilities in:

* Audio Signal Processing
* Feature Engineering (MFCCs)
* Deep Learning with PyTorch
* Real-Time Inference from Microphone Input
* Deployment-ready structure

---

## ğŸš€ Features

âœ” Recognizes emotions from voice in real time
âœ” Uses **LSTM (RNN)** for sequential modeling
âœ” Extracts **MFCC features** using Librosa
âœ” Trained on **RAVDESS Dataset**
âœ” Achieves strong accuracy on test samples
âœ” Modular & clean notebook structure
âœ” Works offline after setup

---

## ğŸ¯ Detected Emotions

The system classifies speech into multiple emotions including:

* Happy
* Sad
* Angry
* Fearful
* Calm
* Neutral
* Disgust
* Surprise

---

## ğŸ§  Model Architecture

* Input: MFCC features (time-series audio representation)
* Model: LSTM-based RNN
* Framework: PyTorch
* Output: Softmax emotion classification

---

## ğŸ“‚ Project Structure

```
emotion_detection/
â”‚
â”œâ”€â”€ data/                     # RAVDESS dataset (not uploaded to repo)
â”œâ”€â”€ training.ipynb            # Model training notebook
â”œâ”€â”€ Real_time_detection.ipynb # Live microphone-based prediction
â”œâ”€â”€ model.pth                 # Trained model weights
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“¥ Dataset

This project uses the **RAVDESS Emotional Speech Audio Dataset**.

Download from Kaggle:
[https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio](https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio)

Extract it to:

```
emotion_detection/data/ravdess/
```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Create Environment

```
python -m venv venv
```

Activate:

Windows:

```
venv\Scripts\activate
```

Linux/Mac:

```
source venv/bin/activate
```

---

### 2ï¸âƒ£ Install Dependencies

```
pip install -r requirements.txt
```

If PyAudio fails on Windows:

```
pip install pipwin
pipwin install pyaudio
```

---

## ğŸ‹ï¸â€â™‚ï¸ Training

Open:

```
training.ipynb
```

Run all cells to:
âœ” Load dataset
âœ” Extract MFCC
âœ” Train LSTM Model
âœ” Save `model.pth`

---

## ğŸ”´ Real-Time Emotion Detection

Open:

```
Real_time_detection.ipynb
```

Run all cells.
Speak into your mic when prompted.
You will see:

```
Recording...
Predicted Emotion: Angry
```

---

## ğŸ“Š Results

* Successfully detects emotions from voice
* Good performance across most classes
* Demonstrates practicality of audio-based affect recognition

(You can add accuracy screenshots or a confusion matrix here later.)

---

## ğŸ§° Tech Stack

* Python
* PyTorch
* Librosa
* NumPy
* SoundDevice / PyAudio
* Scikit-learn
* Jupyter Notebook

---

## ğŸš€ Future Enhancements

Planned improvements:

* Streamlit / Web UI
* Mobile deployment
* Support multilingual datasets
* CNN + Attention architectures
* Noise-robust training

---

## ğŸ‘¤ Author

**Chedalla Harsha**

---

## ğŸ“œ License

This project is licensed under the MIT License.



* Add badges (Stars, License, Python Version, etc.)
* Add screenshots / GIF demo section
* Write a strong â€œProject Highlights for Resumeâ€
* Make README more recruiter-focused

Tell me if you want a **simple**, **premium portfolio**, or **research style** README.
